**DEEP LEARNING BUILDING BLOCKS**
* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), 2014, [49687 citations]

* [Additive Attention: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), ICLR 2015, [13149 citations]

* [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/abs/1312.6120) ICLR2014 [894 citations]

* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909), ACL 2016, 3000 citations **BPE Paper**

* [Layer Normalization](https://arxiv.org/abs/1607.06450), Hinton, 2016, [1959 citations]

* [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022), 2016, [900 citations]

* [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981), 2016, [22 citations]

* [Wordpiece: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144), Google, 2016, (2974 citations)

* [All you need is a good init](https://arxiv.org/abs/1511.06422), ICLR 2016 [386 citations]

* [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412), 2017, [881 citations]

* [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515), NIPS 2017, [991 Citations]

* [L2 Regularization versus Batch and Weight Normalization](https://arxiv.org/abs/1706.05350), 2017, [80 citations]

* [Group Normalization](https://arxiv.org/abs/1803.08494), Kaiming He, 2018 [674 citations]

* [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018, [210 citations]

* [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814), NIPS 2018, [65 citations]

* [Three Mechanisms of Weight Decay Regularization](https://arxiv.org/abs/1810.12281), 2018, [45 citations]

* [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321), [ICLR 2019] [87 Citations]

**NN Architecture**
* [PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS](https://arxiv.org/pdf/1901.10430.pdf), Facebook AI Research

* [Rethinking complex neural network architectures for document classification](https://www.aclweb.org/anthology/N19-1408/), NAACL 2019


**PRE-TRAINING**

* [BAN: Born Again Neural Networks](https://arxiv.org/abs/1805.04770) : one of the initial distillation paper

* [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)

* [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848), 2019

* [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291), 2019

* [Semi-Supervised Sequence Modeling with Cross-View Training](https://arxiv.org/abs/1809.08370), 2018, EMNLP 2018

* [Semi-supervised Multitask Learning for Sequence Labeling](https://arxiv.org/abs/1704.07156)

* [Unsupervised Pretraining for Sequence to Sequence Learning](https://www.aclweb.org/anthology/D17-1039/), EMNLP 2017

* [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH), ICLR 2020

* [Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073), ICLR 2020

* [FreeLB: Enhanced Adversarial Training for Natural Language Understanding](https://arxiv.org/abs/1909.11764), ICLR 2020

* [A Mutual Information Maximization Perspective of Language Representation Learning](https://arxiv.org/abs/1910.08350), ICLR 2020

* [Mogrifier LSTM](https://arxiv.org/abs/1909.01792), ICLR 2020

* [Enhanced LSTM for Natural Language Inference](https://arxiv.org/abs/1609.06038), ACL 2017, 450 citations

**FINETUNING / TRANSFER-LEARNING**

* [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671), ICML 2019, (50 citations)

* [BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis](https://arxiv.org/abs/1904.02232), NAACL 2019 (63 citations)

* [Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling](https://arxiv.org/abs/1812.10860) ACL 2019 (18 citations)

* [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/pdf/1811.01088.pdf), 2018

* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461), 2020

* [Learning and Evaluating General Linguistic Intelligence](https://arxiv.org/pdf/1901.11373.pdf), 2019

* [Learning from Dialogue after Deployment: Feed Yourself, Chatbot!](https://arxiv.org/abs/1901.05415), ACL 2019

* [Syntactic Scaffolds for Semantic Structures](https://arxiv.org/abs/1808.10485), EMNLP 2018

* [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830), ACL 2019

* [Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855), NAACL 2019

* [An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://arxiv.org/abs/1902.10547), NAACL 2019

* [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/abs/1811.01088v2), 2019

* [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/abs/1903.05987), 2019

* [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751), 2019

**MISC**

* [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/abs/1705.00108), ACL 2017

* [A PROBABILISTIC FORMULATION OF UNSUPERVISED TEXT STYLE TRANSFER](https://arxiv.org/pdf/2002.03912.pdf), ICLR 2020


**MULTI-TASK-LEARNNG**

* [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671), ICML 2019

* [Latent Multi-task Architecture Learning](https://arxiv.org/abs/1705.08142), AAAI 2019

**Representation**


* [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf), 2018 [485 citations]

* [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf), 2019, [155 citations]

* [pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference](https://arxiv.org/abs/1810.08854), 2019, NAACL

* [ENCODING WORD ORDER IN COMPLEX EMBEDDINGS](https://arxiv.org/pdf/1912.12333.pdf), ICLR 2020


**NLP Downstream tasks** 

* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635), ICLR 2019

* [Ultra-Fine Entity Typing](https://arxiv.org/abs/1807.04905), ACL 2018 [48 citations]

* [Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing](https://arxiv.org/abs/1903.02591), NAACL 2019

* [Dynamic Meta-Embeddings for Improved Sentence Representations](https://arxiv.org/abs/1804.07983), EMNLP 2018

* [THE CURIOUS CASE OF NEURAL TEXT DeGENERATION](https://arxiv.org/pdf/1904.09751.pdf), ICLR 2020


**INTERPRETABILITY**

* [Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?](https://arxiv.org/abs/2004.03685), ACL 2020, Yoav Goldberg

* [TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS](https://openreview.net/pdf?id=Sy21R9JAW), ICLR 2018

* [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731), ACL 2019, [62 citations]

* [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://www.aclweb.org/anthology/D19-1276.pdf), **Best paper EMNLP 2019**

* [What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document) [Aug-2019] [87 Citations]

* [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284) [ACL-2019] [25 Citations]

* [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf) [ACL-2019] [96 Citations]

* [Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words](https://arxiv.org/abs/2005.01810), ACL 2020

* [Roles and Utilization of Attention Heads in Transformer-based Neural Language Models](https://www.aclweb.org/anthology/2020.acl-main.311/), ACL 2020

* [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://arxiv.org/abs/2004.14786), ACL 2020

* [WHAT CAN NEURAL NETWORKS REASON ABOUT?](https://arxiv.org/pdf/1905.13211.pdf), ICLR 2020

* [TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP](https://arxiv.org/pdf/2005.05909.pdf)

* [Attention in Natural Language Processing](https://arxiv.org/pdf/1902.02181.pdf) Referred by Prof Amit Sheth

**e-Learning**
* Mastering Rate based Curriculum Learning

**Knowledge @ NIPS 2020**
* https://neurips.cc/Conferences/2020/AcceptedPapersInitial
* Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher
* Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning
* Knowledge Augmented Deep Neural Networks for Joint Facial Expression and Action Unit Recognition
* Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control
* Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion
* Learning Retrospective Knowledge with Reverse Reinforcement Learning
* Learning Deep Attribution Priors Based On Prior Knowledge
* Searching Recurrent Architecture for Path-based Knowledge Graph Embedding
* BoxE: A Box Embedding Model for Knowledge Base Completion
* Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space
* Algorithmic recourse under imperfect causal knowledge: a probabilistic approach
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
* Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability
* Dynamic Fusion of Eye Movement Data and Verbal Narrations in Knowledge-rich Domains
* Faithful Embeddings for Knowledge Base Queries
* Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs
* Group Knowledge Transfer: Collaborative Training of Large CNNs on the Edge
* Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models
* What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge
* A Benchmark for Systematic Generalization in Grounded Language Understanding
* Towards Understanding Hierarchical Learning: Benefits of Neural Representations
* Billion-scale similarity search with gpus.
* Well-Read Students Learn Better: On the Importance of Pre-training Compact Models
* An Algorithm for Learning Smaller Representations of Models With Scarce Data [Agora]
* An Approximation Algorithm for Optimal Subarchitecture Extraction [FPTAS]
* 

 * The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence
* REALM: Retrieval-Augmented Language Model Pre-Training
* Dense passage retrieval for open-domain question answering. (DPR) 
* How Context Affects Language Models' Factual Predictions
* Latent retrieval for weakly supervised open domain question answering - ACL 2019

**Explainability in NIPS2020**
* How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods [DONE]
* Towards Interpretable Natural Language Understanding with Explanations as Latent Variables [DONE - Summary Pending]
* Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models
* Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability
* Margins are Insufficient for Explaining Gradient Boosting
* What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes
* PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks
* Implicit Regularization in Deep Learning May Not Be Explainable by Norms
* Model Interpretability through the lens of Computational Complexity
* Attribution Preservation in Network Compression for Reliable Network Interpretation
* GANSpace: Discovering Interpretable GAN Controls
* Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses
* How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions
* Benchmarking Deep Learning Interpretability in Time Series Predictions
* ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping
* Regularizing Black-box Models for Improved Interpretability
* Fourier-transform-based attribution priors improve the interpretability and stability of deep learning models for genomics
* Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE
* Learning outside the Black-Box: The pursuit of interpretable models
* Incorporating Interpretable Output Constraints in Bayesian Neural Networks
* Interpretable Sequence Learning for Covid-19 Forecasting


**Question Generation**
* Machine Comprehension by Text-to-Text Neural Question Generation
* CopyBERT: A Unified Approach to Question Generation with Self-Attention
** BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model
** Unified language model pre-training for natural language understanding and generation. 
* A Systematic Review of Automatic Question Generation for Educational Purposes
* Difficulty-controllable multi-hop question generation from knowledge graphs, 2019

**Question Answering**
* Simple and Effective Multi-Paragraph Reading Comprehension 

**KG + NLP**

* SCITAIL: A Textual Entailment Dataset from Science Question Answering

* Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering

* EXPLOITING STRUCTURED KNOWLEDGE IN TEXT VIA GRAPH-GUIDED REPRESENTATION LEARNING

* WorldTree V2: A Corpus of Science-Domain Structured Explanations and Inference Patterns supporting Multi-Hop Inference

* SemEval-2020 Task 4: Commonsense Validation and Explanation

* ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous Knowledge Resources for Commonsense Validation and Explanation

**Best NLP Papers ACL 2019 & 2020**

* [How Can We Accelerate Progress Towards Human-like Linguistic Generalization?](https://arxiv.org/pdf/2005.00955.pdf)

**Suggestions from Prof Amit Sheth**
* [Attention in Natural Language Processing](https://arxiv.org/pdf/1902.02181.pdf) 2020

**Medhas Discussion** 



